# OpenTelemetry Tracing for DSPy Server

The DSPy CLI server includes built-in support for OpenTelemetry tracing using the [OpenInference DSPy instrumentation](https://github.com/Arize-ai/openinference).

## Quick Start

### 1. Enable Tracing

Set the `OTEL_TRACING_ENABLED` environment variable:

```bash
export OTEL_TRACING_ENABLED=true
```

### 2. Start the Trace Viewer (Optional)

If you want to view traces locally, start the DSPy trace viewer:

```bash
cd trace-viewer
pip install -r requirements.txt
python viewer.py
```

This starts a local trace viewer at http://localhost:4318

### 3. Run Your DSPy Server

```bash
dspy-cli serve
```

### 4. View Traces

Open http://localhost:4318 in your browser to see traces of all DSPy executions.

## Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `OTEL_TRACING_ENABLED` | `false` | Enable/disable tracing |
| `OTEL_SERVICE_NAME` | `dspy-server` | Service name in traces |
| `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT` | `http://localhost:4318/v1/traces` | OTLP endpoint URL |

### Example Configuration

```bash
# Enable tracing with custom service name
export OTEL_TRACING_ENABLED=true
export OTEL_SERVICE_NAME=my-rag-api
export OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://localhost:4318/v1/traces

# Run server
dspy-cli serve
```

## What Gets Traced?

The instrumentation automatically captures:

- **LLM Calls**: Model name, prompts, completions, token usage
- **Chain Execution**: Input/output of Predict, ChainOfThought, etc.
- **Retrieval**: Queries, retrieved documents, scores
- **Tool Calls**: Tool names, arguments, results
- **Timing**: Duration of each operation

### Span Hierarchy Example

```
POST /MyRAG
└── MyRAG.forward (CHAIN)
    ├── Retrieve.forward (RETRIEVER)
    │   └── ColBERTv2.__call__ (RETRIEVER)
    └── ChainOfThought.forward (CHAIN)
        └── LM.__call__ (LLM)
            └── OpenAI API call
```

## Use Cases

### Local Development & Debugging

See exactly what prompts are being sent to your LLM and what responses come back:

```bash
export OTEL_TRACING_ENABLED=true
dspy-cli serve
```

Then make requests and watch traces appear in real-time.

### Performance Analysis

Identify slow operations in your DSPy pipeline:

- Which retrievals are taking longest?
- How much time is spent in LLM calls vs processing?
- Where are the bottlenecks in your chain?

### Prompt Engineering

View the actual prompts generated by DSPy:

1. Enable tracing
2. Make a request
3. Open trace viewer
4. Expand LLM spans to see full prompts

## Sending Traces to Other Backends

### Phoenix (Arize)

```bash
# Start Phoenix
python -m phoenix.server.main serve

# Configure endpoint
export OTEL_TRACING_ENABLED=true
export OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://localhost:6006/v1/traces

dspy-cli serve
```

### Jaeger

```bash
# Start Jaeger all-in-one
docker run -d --name jaeger \
  -p 4318:4318 \
  -p 16686:16686 \
  jaegertracing/all-in-one:latest

# Configure endpoint
export OTEL_TRACING_ENABLED=true
export OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://localhost:4318/v1/traces

dspy-cli serve
```

View traces at http://localhost:16686

### Custom OTLP Collector

```bash
export OTEL_TRACING_ENABLED=true
export OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=https://your-collector.example.com/v1/traces

dspy-cli serve
```

## Trace Attributes

The OpenInference instrumentation adds semantic attributes following the [OpenInference specification](https://github.com/Arize-ai/openinference/blob/main/spec/semantic_conventions.md):

### LLM Spans

- `openinference.span.kind`: `LLM`
- `llm.model_name`: Model identifier
- `llm.provider`: Provider (e.g., `openai`)
- `llm.input_messages`: Array of messages
- `llm.output_messages`: Response messages
- `llm.token_count.prompt`: Input tokens
- `llm.token_count.completion`: Output tokens
- `llm.token_count.total`: Total tokens

### Chain Spans

- `openinference.span.kind`: `CHAIN`
- `input.value`: JSON-encoded inputs
- `output.value`: JSON-encoded outputs

### Retriever Spans

- `openinference.span.kind`: `RETRIEVER`
- `retrieval.documents`: Array of retrieved documents
  - `document.content`: Document text
  - `document.id`: Document ID
  - `document.score`: Relevance score

### Tool Spans

- `openinference.span.kind`: `TOOL`
- `tool.name`: Tool name
- `tool.description`: Tool description
- `input.value`: Tool arguments
- `output.value`: Tool result

## Troubleshooting

### Traces Not Appearing

1. **Check tracing is enabled**:
   ```bash
   echo $OTEL_TRACING_ENABLED
   # Should output: true
   ```

2. **Check server logs**:
   Look for: `✓ OpenTelemetry tracing enabled: dspy-server -> http://...`

3. **Verify endpoint is reachable**:
   ```bash
   curl http://localhost:4318/api/stats
   # Should return JSON with trace viewer stats
   ```

4. **Check for errors in logs**:
   Look for warnings about missing dependencies or connection errors

### Missing Dependencies

If you see import errors, install the tracing dependencies:

```bash
pip install openinference-instrumentation-dspy opentelemetry-sdk opentelemetry-exporter-otlp-proto-http
```

Or reinstall dspy-cli with the latest dependencies:

```bash
pip install -e .
```

### High Overhead

Tracing adds some latency to each request. For production:

1. Use batch span processors instead of simple:
   ```python
   # In app.py, replace SimpleSpanProcessor with:
   from opentelemetry.sdk.trace.export import BatchSpanProcessor
   tracer_provider.add_span_processor(
       BatchSpanProcessor(OTLPSpanExporter(endpoint=otlp_endpoint))
   )
   ```

2. Disable tracing for high-throughput endpoints

3. Use sampling to only trace a percentage of requests

## Disabling Tracing

To disable tracing, simply unset or set to false:

```bash
export OTEL_TRACING_ENABLED=false
# or
unset OTEL_TRACING_ENABLED
```

## See Also

- [OpenInference DSPy Instrumentation](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-dspy)
- [OpenTelemetry Python](https://opentelemetry.io/docs/instrumentation/python/)
- [DSPy Trace Viewer](trace-viewer/README.md)
- [Phoenix](https://github.com/Arize-ai/phoenix)
